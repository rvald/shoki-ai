import os
import json
from typing import List
from openai import OpenAI

def evaluate_model(
    inputs: List[str], 
    outputs: List[str], 
    model_responses: List[str], 
    model: str,
    experiment_name: str = "default_experiment"
):
    """
    Evaluates model responses and stores results in ./experiments/experiment_name.json
    """
    
    SYSTEM_PROMPT = """
    You are an impartial evaluator. Your task is to assess the response generated by a model, given the original input and the expected output. Evaluate how well the response matches the expected output in terms of accuracy, completeness, and relevance.
    Score the response on a scale of 1-5, where 1 means "completely incorrect or irrelevant," and 5 means "fully correct, complete, and relevant."
    Justify your score with a brief explanation, focusing on specific differences or similarities between the model's response and the expected output.
    Output your evaluation in the following JSON format:
    {
        "input": "[original input]",
        "expected_output": "[expected output]",
        "model_response": "[model's response]",
        "score": [1-5],
        "explanation": "[brief rationale for the score]"
    }
    Carefully read and compare the expected output with the modelâ€™s response before assigning a score and writing your explanation. Respond with only the completed JSON object.
    """

    # Store experiment results
    results = []

    # Get your API key securely
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    client = OpenAI(api_key=OPENAI_API_KEY)

    # Evaluate each response
    for inp, exp_out, model_resp in zip(inputs, outputs, model_responses):
        user_content = (
            f"Evaluate the response of the model based on the input: {inp} "
            f"and the expected output: {exp_out}. "
            f"Here is the model response: {model_resp}"
        )
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_content},
            ],
        )
        # Parse and store JSON result from response
        output_json = response.choices[0].message.content.strip()
        try:
            results.append(json.loads(output_json))
        except:
            print("Failed to parse JSON for input:", inp)
            print("Raw output:", output_json)

    # Ensure experiments directory exists
    os.makedirs("experiments/evaluations", exist_ok=True)
    output_path = os.path.join("experiments", "evaluations", f"{experiment_name}.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"Results stored in {output_path}")